<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en"><head>
  <title>Quasimetric RL (QRL)</title>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

<meta property="og:image" content="./images/function_spaces.png"/>
<meta property="og:title" content="Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning" />
<script src="https://cdn.jsdelivr.net/gh/google/code-prettify@master/loader/run_prettify.js"></script>

<script src="lib.js" type="text/javascript"></script>
<script src="popup.js" type="text/javascript"></script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>

<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

<script type="text/javascript">
// redefining default features
var _POPUP_FEATURES = 'width=500,height=300,resizable=1,scrollbars=1,titlebar=1,status=1';
</script>
<link media="all" href="glab.css" type="text/css" rel="StyleSheet">
<style type="text/css" media="all">
IMG {
	PADDING-RIGHT: 0px;
	PADDING-LEFT: 0px;
	FLOAT: right;
	PADDING-BOTTOM: 0px;
	PADDING-TOP: 0px
}
#primarycontent {
	MARGIN-LEFT: auto; ; WIDTH: expression(document.body.clientWidth >
1000? "1000px": "auto" ); MARGIN-RIGHT: auto; TEXT-ALIGN: left; max-width:
1000px }
BODY {
	TEXT-ALIGN: center
}
</style>

<style type="text/css">
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    margin-bottom: 0px;
    width: 100%;
  }

  h1 {
    font-weight:300;
  }

  p {
    font-size: 16px;
  }

  div {
    max-width: 95%;
    margin:auto;
    padding: 10px;
  }

  .table-like {
    display: flex;
    flex-wrap: wrap;
    flex-flow: row wrap;
    justify-content: center;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img {
    padding: 0;
    display: block;
    margin: 0 auto;
    max-height: 100%;
    max-width: 100%;
  }

  iframe {
    max-width: 100%;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    max-width: 1100px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  #authors td {
    padding-bottom:5px;
    padding-top:30px;
  }
</style>

<body>

<div id="primarycontent">

<center><h1 style="font-size: 225%"><span style="font-weight: bold;">Optimal</span> Goal-Reaching Reinforcement Learning via <span style="font-weight: bold;">Quasimetric</span> Learning</h1></center>
<center>
  <div class="table-like" style="justify-content:space-evenly;max-width:880px;margin:auto;">
    <div width="1"></div>
    <div>
      <center>
        <a href=".." style="font-size: larger">Tongzhou Wang</a>
      </center>
      <center>
        MIT CSAIL
      </center>
    </div>
    <div width="1"></div>
    <div>
      <center>
        <a href="https://web.mit.edu/torralba/www/" style="font-size: larger">Antonio Torralba</a>
      </center>
      <center>
        MIT CSAIL
      </center>
    </div>
    <div width="1"></div>
    <div>
      <center>
        <a href="https://web.mit.edu/phillipi/" style="font-size: larger">Phillip Isola</a>
      </center>
      <center>
        MIT CSAIL
      </center>
    </div>
    <div width="1"></div>
    <div>
      <center>
        <a href="https://amyzhang.github.io/" style="font-size: larger">Amy Zhang</a>
      </center>
      <center>
        UT Austin, Meta AI
      </center>
    </div>
    <div width="1"></div>
  </div>
</center>


<h3 style="text-align:center; font-size:110%; margin-top:5px;">
    <!-- Workshop on Symmetry and Geometry in Neural Representations at NeurIPS 2022<br/>
    PMLR Proceedings -->
</h3>

<div class="table-like" style="justify-content:space-evenly;max-width:900px;margin:auto;margin-top:10px">
  <center>
    <table>
      <tr>
        <td style="font-size:20px;margin:20px;font-family:monospace;">
          <a style="margin:2px;height: 100%;display:flex;align-items: center;" href="assets/paper.pdf">
          <img style="float: left; max-width: 120%;padding-right: 10px;" alt="paper thumbnail" src="assets/images/paper_thumbnail.png" width=60>
          [arXiv (FIXME)]
          </a>
        </td>
        <td style="padding-left:60px">
          <div width="2"></div>
        </td>
        <td style="font-size:20px;margin:20px;font-family:monospace">
          <div style="margin:2px;height: 100%;display:flex;align-items: center;" href="javascript:void(0);">
          <img style="float: left; max-width: 120%;padding-right: 10px;" alt="paper thumbnail" src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" width=60>
          [Code Coming Soon]
          </div>
          <!-- <a style="margin:2px" href="https://github.com/quasimetric-learning/quasimetric_rl/">[Code]</a> -->
        </td>
      </tr>
    </table>
  </center>
</div>

<center>
<div class="table-like" style="justify-content:space-evenly;max-width:1000px;width:1000px;margin:auto;margin-top:10px;padding: 0px;">
    <!-- <table style="width: calc(100% - 8px);  border: 2px solid #000;  display: inline-block;  padding: 2px;"> -->
    <table style="width: calc(100% - 8px);  border: 2px solid #000;  display: inline-block;  padding: 2px;margin:2px">
    <tr>
      <td colspan="5" style="padding-bottom: 17px;font-size:25px;text-align: center;">Overview of Quasimetric RL (QRL)</td>
    </tr>
      <tr style="width: 100%;text-align: center;">
        <td style="font-size:17.5px;font-family:monospace;display: inline-block;text-align: center;width:35%;padding: 0px;">
          <img style="float: left; width: 100%;padding-bottom: 10px;" alt="paper thumbnail" src="assets/images/quasimetric_structure.png">
          Quasimetric Geometry
          </a>
        </td><td style="font-size:40px;font-family:monospace;display: inline-block;text-align: center;width:4%;padding: 0px;">
          +
        </td><td style="font-size:17.5px;font-family:monospace;display: inline-block;text-align: center;vertical-align: bottom;width:36%;padding: 0px;"><div style="padding-bottom: 8px;" >
          <video src="https://user-images.githubusercontent.com/5674597/229619483-4e565dee-7b69-45a6-8f81-f21647f0df71.mp4" controls="controls" autoplay="true" loop="true" muted="muted" class="d-block rounded-bottom-2 border-top width-fit" style="width:100%">  </video></div>
          A Novel Objective<br>
          <span style="font-size: 13.5px;">(Push apart <span style="color:red;font-weight: 1000;">start state</span> and <span style="color:red;font-weight: 1000;">goal</span><br>while maintaining local distances)</span>
          </a>
        </td><td style="font-size:40px;font-family:monospace;display: inline-block;text-align: center;width:4%;padding: 0px;">
          =
        </td><td style="font-size:19px;font-family:monospace;display: inline-block;text-align: center;width:19.5%;padding: 0px;">
          Optimal Value $V^*$<br><span style="color:#97999c; font-weight: 200;font-style: italic;">AND</span><br> High-Performing Goal-Reaching Agents
        </td>
      </tr>
    </table>
</div>
<br>

<h2>Abstract</h2>
<div style="font-size:14px; text-align: justify;max-width: 92%;">
<p>
In goal-reaching reinforcement learning (RL), the optimal value function has a particular geometry, called <span style="font-weight: bold;">quasimetric structure</span> (<a style="text-decoration: underline;" href="../quasimetric/">see</a> <a style="text-decoration: underline;" href="../interval_quasimetric_embedding/">also</a>  <a style="text-decoration: underline;" href="https://arxiv.org/abs/2002.05825">these</a>  <a style="text-decoration: underline;" href="https://arxiv.org/abs/2208.08133">works</a>). This paper introduces <em style="font-weight: bold;">Quasimetric Reinforcement Learning (QRL)</em>, a new RL method that utilizes quasimetric models to learn <span style="font-weight: bold;">optimal</span> value functions. Distinct from prior approaches, the QRL objective is specifically designed for quasimetrics, and provides strong theoretical recovery guarantees. Empirically, we conduct thorough analyses on a discretized <code>MountainCar</code> environment, identifying properties of QRL and its advantages over alternatives. On offline and online goal-reaching benchmarks, QRL also demonstrates improved sample efficiency and performance, across both state-based and image-based observations.
</p>
</div>

<hr style="margin-bottom: 15px;">
<h2>Optimal Value Functions are Quasimetrics</h2>
<br>

 <style type="text/css">
   .tooltip {
   /*display: inline-block;*/
   padding-bottom: 0px;
   /*border-bottom: 1px dotted black;*/
 }

 .tooltip .tooltipdetails {
   display: block;                                                                                                                              visibility: hidden;
   width: 980px;
   color: black;
   text-align: left;
   border-radius: 6px;
   padding: 5px 0;
   position: absolute;
   z-index: 1;
   border:3px solid black;                                                                                                                      /*bottom: 100%;*/
   /*left: 0%;*/
   /*margin-left: -60px;*/
   background-color:white;
   opacity: 0;
   transition: opacity 1s;
 }

 .tooltip:hover .tooltipdetails {
   visibility: visible;
   opacity: 1;
 }
 </style>


<table border="0" cellspacing="0" cellpadding="0">
  <tr>
    <td align="center" valign="bottom" style="overflow:hidden;">
        <img class="result" src="../quasimetric/images/function_spaces_cropped.png" style="width: 95%">
    </td>
  </tr>
</table>

<div style="text-align: justify;font-size: 16px;max-width:97.5%">
  <em style="font-weight: bold;">Quasimetrics</em> are a generalization of metrics in that they do not require symmetry. They are well suited for characterizing  <span style="font-weight: bold;">optimal cost-to-go (value function) in goal-reaching tasks</span> that generally have
   <span class="tooltip" style="text-align: left;padding: 0px;margin-left: 0px; margin-right: 0px;border-bottom: 1px dotted black; max-width: 100%;">asymmetrical dynamics<span class="tooltipdetails" style="padding:0px"><div style="font-size: 14px; max-width: 100%; padding-top: 7px">
      E.g., due to time, gravity, actions with irreversible consequences, asymmetrical rules (such as one-way-roads), etc.
     </div></span></span> and where
   <span class="tooltip" style="text-align: left;padding: 0px;margin-left: 0px; margin-right: 0px;border-bottom: 1px dotted black; max-width: 100%;">triangle-inequality inherently holds.
   <span class="tooltipdetails" style="padding:0px">
     <div style="font-size: 14px; max-width: 100%; padding-top: 7px">
       The <span style="font-weight: bold;">optimal</span> path from $A$ to $C$ is at least as good as the optimal path from $A$ to $B$ and then to $C$.
     </div>
   </span>
   </span>
</div>


<table border="0" cellspacing="0" cellpadding="0">
  <tr>
    <td align="center" valign="bottom" style="overflow:hidden;">
        <img class="result" src="assets/images/quasimetric_properties.png" style="width: 75%;margin:12px">
    </td>
  </tr>
</table>


<div style="text-align: justify;font-size: 16px;max-width:97.5%">
We formalize this in Theorem 1 of our paper, stating that the space of quasimetrics is the <span style="font-weight: bold;">exact</span> function class for goal-reaching RL:
$$\textsf{Quasimetrics on state space }\mathcal{S}\equiv\{-V^* \colon V^* \textsf{ is the optimal goal-reaching value of an MDP on }\mathcal{S}\},$$
where $V^*(s; \textsf{goal}=s_g)$ is the optimal cost-to-go of an MDP from state $s \in \mathcal{S}$ to goal $s_g \in \mathcal{S}$.
</div>



<hr style="margin-bottom: 15px;">
<h2>Quasimetric Models for Learning Value Functions</h2>
<br>

<table border="0" cellspacing="0" cellpadding="0">
  <tr>
    <td align="center" valign="bottom" style="overflow:hidden;">
        <img class="result" src="assets/images/qmet_model_to_what_to_vstar.png" style="width: 95%">
    </td>
  </tr>
</table>
<br/>


<div style="text-align: justify;font-size: 16px;max-width:97.5%">
Recently proposed <em style="font-weight: bold;"><a style="text-decoration: underline;" href="../interval_quasimetric_embedding">quasimetric</a> <a style="text-decoration: underline;" href='../quasimetric'>models</a></em> are parametrized models (based on neural networks) $\{d_\theta\}_\theta$ that <lu>
  <li style="margin-left: 30px;margin-bottom: 3px;"> $d_\theta$ is a quasimetric (i.e., always satisfying triangle-inequality).
  </li>
  <li style="margin-left: 30px;margin-bottom: 3px"> $\{d_\theta\}_\theta$ can approximate <span style="font-weight: bold;">any</span> quasimetric function.
  </li>
</lu>
Quasimetric models are thus perfect choices for parametrizing the optimal goal-reaching values $V^*(s; \textsf{goal}=s_g)$ in RL.
</div>

<div style="text-align: justify;font-size: 16px;max-width:97.5%">
  One can simply plug such quasimetric models into standard RL algorithms (e.g., Q-learning and DDPG), <a style="text-decoration: underline;" href="../quasimetric/">as</a>  <a style="text-decoration: underline;" href="../interval_quasimetric_embedding">these</a> <a style="text-decoration: underline;" href="https://arxiv.org/abs/2002.05825">works</a>  <a style="text-decoration: underline;" href="https://arxiv.org/abs/2208.08133">do</a>. However, the convergence of such algorithms is <span style="font-weight: bold;">not guaranteed</span>, as
   <span class="tooltip" style="text-align: left;padding: 0px;margin-left: 0px; margin-right: 0px;border-bottom: 1px dotted black; max-width: 100%;">the intermediate results may not be quasimetric functions and cannot be represented by quasimetric models<span class="tooltipdetails" style="padding:0px">
     <div style="font-size: 14px; max-width: 100%; padding-top: 7px">These algorithms generally rely on policy iteration, temporal-difference learning, or iterative Bellman updates, all of which do converge to optimal $V^*$ <span style="font-weight: bold;">with an unconstrained function class</span>, thanks to its capability of representing on-policy values or Bellman update outputs.
      However, there is no guarantee that such intermediate results are quasimetrics (see our Theorem 1), and thus the standard convergence results do not directly extend to value functions parametrized by <span style="font-weight: bold;">quasimetric models</span> which can only model quasimetrics.</div></span></span>.
   Therefore, benefits can be <span class="tooltip" style="text-align: left;padding: 0px;margin-left: 0px; margin-right: 0px;border-bottom: 1px dotted black; max-width: 100%;">rather minor<span class="tooltipdetails" style="padding:0px"><div style="font-size: 14px; max-width: 100%; padding-top: 7px">E.g., see offline Q-learning results in <a style="text-decoration: underline;" href="../quasimetric/">these</a>  <a style="text-decoration: underline;" href="../interval_quasimetric_embedding">works</a>.</div></span></span>,   and sometimes
   <span class="tooltip" style="text-align: left;padding: 0px;margin-left: 0px; margin-right: 0px;border-bottom: 1px dotted black; max-width: 100%;">quasimetric constraints are relaxed.
   <span class="tooltipdetails" style="padding:0px">
     <div style="font-size: 14px; max-width: 100%; padding-top: 7px">
      E.g.,  <a style="text-decoration: underline;" href="https://arxiv.org/abs/2208.08133">this work</a> uses quasimetric models in a way that doesn't strictly restrict the value function to be a quasimetric.
     </div>
   </span>
   </span>
</div>


<hr style="margin-bottom: 15px;">
<h2>Quasimetric Reinforcement Learning (QRL)</h2>
<br>

<table border="0" cellspacing="0" cellpadding="0">
  <tr>
    <td align="center" valign="bottom" style="overflow:hidden;">
        <img class="result" src="assets/images/qrl_to_vstar.png" style="width: 95%">
    </td>
  </tr>
</table>
<br/>

<div style="text-align: justify;font-size: 16px;max-width:97.5%">
Our work instead designs a <span style="font-weight: bold;">new RL algorithm specifically for quasimetric models</span>, which is called <span style="font-weight: bold;">Quasimetric Reinforcement Learning (QRL)</span>.  In contrast with other RL approaches, QRL directly <span style="font-weight:bold">learns the optimal value function $V^*$</span> (without policy iteration or temporal-difference learning). <lu>
  <li style="margin-left: 30px;"> QRL uses a <span style="text-decoration: underline;">quasimetric model </span> $d_\theta$ to parametrize $-V^*$, restricting the search space to quasimetrics that satisfies <span style="text-decoration: underline;">triangle-inequality</span>.
  </li>
  <li style="margin-left: 30px;"> QRL enforces that $V^*$ <span style="text-decoration: underline;">accurately capture the local transition costs/distances</span>: $$\forall\ \mathsf{transition}\ (s, a, s', \mathsf{cost}),\quad V^*(s; s') = -\mathsf{cost}
  \tag{$\mathsf{reward} = - \mathsf{cost}$}$$
  </li>
  <li style="margin-left: 30px;"> <span style="text-decoration: underline;">Subject to the above constraint</span>, QRL <span style="text-decoration: underline;">maximally pushes apart the estimated distance/cost between any two states</span>: $$\max_{V^*} \mathbb{E}_{\mathsf{random}\ s_0, s_1} [-V^*(s_0; s_1)]
  \tag{$-V^* \textsf{ estimates the cost/distance}$}$$
</lu>
</div>

<div style="text-align: justify;font-size: 16px;max-width:97.5%">
In summary, QRL optimizes the following objective:$$
    \max_{\theta}~\mathbb{E}_{\substack{s\sim p_\mathsf{state}\\g \sim p_\mathsf{goal}}}[ d_\theta(s, g) ] \qquad \text{subject to }\ \mathbb{E}_{\substack{(s, a, s') \sim p_\mathsf{transition}}}[ (\underbrace{d_\theta(s, s') - 1}_{\llap{\textsf{assume constant cost for}}\rlap{\textsf{ simplicity (can be relaxed)}}})^2] \leq\overbrace{\epsilon^2}^{\llap{\epsilon\textsf{ is a }}\rlap{\textsf{small positive constant}}}  \tag{QRL}
$$
</div>

<div style="text-align: justify;font-size: 16px;max-width:97.5%">
<span style="font-weight: bold;">Why QRL learns optimal $V^*$.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span> Consider <span style="font-weight: bold;color: rgb(217, 0, 0);">two objects</span> connected by <span style="font-weight: bold;color: rgb(100, 100, 100);">multiple chains</span> (video below), where each chain is formed by serveral <span style="text-decoration: underline;">non-deformable links (i.e., each link is a "local transition" with a fixed length/cost)</span>.  If we <span style="text-decoration: underline;">maximally push the <span style="font-weight: bold;color: rgb(217, 0, 0);">two objects</span> apart</span>, the distance will be limited by the shortest of all chains due to the <span style="text-decoration: underline;">triangle-inequality</span> of our Euclidean physical space. Then, simply measuring the <span style="font-weight: bold;color: rgb(217, 0, 0);">distance between the two objects gives the length of that "optimal" chain connecting them</span>.
</div>

<video src="https://user-images.githubusercontent.com/5674597/229381754-0ef53e15-b6bb-4b0a-a639-345d241d1546.mp4" controls="controls" autoplay="true" loop="true" muted="muted" class="d-block rounded-bottom-2 border-top width-fit" style="width:92.5%">  </video>

<div style="font-size:14px; text-align: justify;padding-top: 0px;margin-left: 0px; margin-right: 0px;max-width: 100%">
<center><span style="font-weight: bold;">Two examples of pushing <span style="font-weight: bold;color: rgb(217, 0, 0);">two pairs of nodes</span> apart.</span> After pushing, the distance (optimal cost) between them can be directly read out.</center>
</div>

<div style="text-align: justify;font-size: 16px;max-width:97.5%">
<span style="font-weight: bold;">QRL</span> works exactly in this way, except that it simultaneously pushes apart all pairs in a quasimetric space that can capture the possibly asymmetrical MDP dynamics. Our paper presents <span style="font-weight: bold;">rigorous theoretical guarantees that QRL recovers of $V^*$</span> (Theorems 2 and 3).
</div>

<div style="text-align: justify;font-size: 16px;max-width:97.5%">
See our paper on how to go from this $V^*$ estimate to a $Q^*$ estimate, and then to a policy.
</div>

<hr>
<h2>QRL Accurately Recovers $V^*$</h2>

<h3 style="text-align: left;font-size: 20px;padding-left: 20px; padding-top:10px; padding-bottom: 0px">Offline Learning on Discretized <code>MountainCar</code></h3>
<table border="0" cellspacing="0" cellpadding="0">
  <tr>
    <td align="center" valign="bottom" style="overflow:hidden;padding-top:8px">
        <img class="result" src="assets/images/val_fn_clean.png" style="width: 95%">
    </td>
  </tr>
</table>

<div style="text-align: justify;font-size: 16px;padding-top: 10px;max-width:97.5%;">
Visualizations of learned value functions. Only QRL recovers the detailed  structure of the ground truth $V^*$ (left section).

<span class="tooltip" style="text-align: left;padding: 0px;margin-left: 0px; margin-right: 0px;border-bottom: 1px dotted black; max-width: 100%;">Hover for details.
<span class="tooltipdetails" style="padding:0px">
  <div style="font-size: 14px; max-width: 100%; padding-top: 7px">
  Each plot shows the estimated values from every state towards a single goal (indicated in leftmost column) as a 2-dimensional image (velocity as $x$-axis, position as $y$-axis).
  </div>
  <div style="font-size: 14px; max-width: 100%; padding-top: 0px">
  <span style="font-weight:bold">Left:</span> Ground truth distances, as well as the (expected) distance for the behavior policy that generated dataset.

  </div>
  <div style="font-size: 14px; max-width: 100%; padding-top: 0px">
  <span style="font-weight:bold">Middle:</span> Learned value functions for single-goal methods.
  </div>
  <div style="font-size: 14px; max-width: 100%; padding-top: 0px">
  <span style="font-weight:bold">Right:</span> Learned value functions for multi-goal methods.
  </div>
  <div style="font-size: 14px; max-width: 100%; padding-top: 0px">
  </div>
  <div style="font-size: 14px; max-width: 100%; padding-top: 0px">

  <span style="font-weight:bold">Only QRL attains accurately recovers the ground truth distance structure</span>, which crucially relies on the asymmetry of quasimetrics. Q-learning methods generally fail. While their learned values improve with quasimetric models, they still can't capture the fine details. Contrastive RL only inaccurately estimates the <span style="font-weight:bold">on-policy</span> values.
  </div>
</span>
</span>
</div>

<div style="text-align: justify;font-size: 16px;max-width:97.5%">
See our paper for control results on this environment.
</div>


<hr>
<h2>QRL Quickly Finds High-Quality Policies in Both Offline and Online RL</h2>

<h3 style="text-align: left;font-size: 20px;padding-left: 20px; padding-top:15px; padding-bottom: 0px">Offline Goal-Reaching <code>maze2d</code><span style="font-weight: normal;"> (Normalized Scores)</span></h3>
<table border="0" cellspacing="0" cellpadding="0">
  <tr>
    <td align="center" valign="bottom" style="overflow:hidden;padding-top:8px">
        <img class="result" src="assets/images/qrl_maze2d.png" style="width: 97.5%">
    </td>
  </tr>
</table>
<div style="text-align: justify;font-size: 16px;padding-top: 0px;max-width:97.5%;">
QRL outperforms SOTA offline RL methods, a trajectory modelling method, and Contrastive RL. QRL's learned $V^*$ estimate can be directly used to improve planning (MPPI) and trajectory sampling (Diffuser guided sampling).
</div>
<br/>


<h3 style="text-align: left;font-size: 20px;padding-left: 20px; padding-bottom: 0px">Online Goal-Conditional RL Benchmarks with State-based and Image-based Observations</h3>
<table border="0" cellspacing="0" cellpadding="0">
  <tr>
    <td align="center" valign="bottom" style="overflow:hidden;padding-top:8px">
        <img class="result" src="assets/images/online_final_arXiv.png" style="width: 97.5%">
    </td>
  </tr>
</table>
<div style="text-align: justify;font-size: 16px;padding-top: 0px;max-width:97.5%;">
QRL outperforms simply using quasimetric models (MRN or IQE) in DDPG, suggesting the effectiveness of the QRL objective.
</div>
<br/>

<hr>
<table>
  <tr>
    <td align="center" colspan="2" valign="bottom" style="overflow:hidden;padding-top:8px;width:100%">
    <a href="https://arxiv.org/abs/2211.15120">
        <img class="result" src="assets/images/paper_horizontal.png" style="width: calc(100% - 8px);  border: 2px solid #000;  display: inline-block;  padding: 2px;">
    </a>
    </td>
  </tr>
  <tr>
    <!-- <td style="padding: 10px; padding-right: 50px">
      <span>
      <a href="https://arxiv.org/abs/2211.15120"><img style="float: left; max-width: 120%" alt="paper thumbnail" src="assets/images/paper_thumbnail.png" width=200></a>
      </span>
    </td> -->
    <td>
      <span>
        <h2>Paper</h2>
        <p>
          <a href="https://arxiv.org/abs/2211.15120">arXiv (FIXME)</a> 2023.
        </p>

        <h2>Citation</h2>
        <p>Tongzhou Wang, Antonio Torralba, Phillip Isola, Amy Zhang. "Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning" <em>arXiv</span>. 2023.
        </p>
        <!-- <h2>Code:<span style="font-family:monospace;font-size:21px;margin:5px;position:relative;bottom:2px">
            <a style="font-weight:normal;" href="https://github.com/quasimetric-learning/torch-quasimetric">[PyTorch Package of SOTA Quasimetric Learning Methods]</a>
          </span></h2> -->
        <br>
      </span>
    </td>
  </tr>
</table>

<h3 style="margin-top: -1.6em;text-align:left"><code style="font-size: 15pt">bibtex</code> <span style="font-size: 14.5pt">entry</span></h3>
<div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;padding-right: 4em;width: 95%">
<pre style="font-size: 10pt; margin: .3em 0px;text-align: left">
@article{tongzhouw2023qrl,
  title={Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning},
  author={Wang, Tongzhou and Torralba, Antonio and Isola, Phillip and Zhang, Amy},
  journal={arXiv preprint arXiv:FIXME},
  year={2023}
}
</pre>

</div>

<style type="text/css" media="all">
.page__footer {
  /*float: left;*/
  padding-top: 1em;
  padding-bottom: 0.5em;
  margin-left: 0;
  margin-right: 0;
  width: 100%;
  clear: both;
  /* sticky footer fix start */
  /*position: absolute;*/
  bottom: 0;
  height: auto;
  /* sticky footer fix end */
  margin-top: 3em;
  color: #898c8f;
  background-color: #f2f3f3;
  padding-left: 0em;
  padding-right: 0em;
  max-width: 100%;
}

.page__footer .links {
  margin-left: auto;
  margin-right: auto;
  max-width: 1000px;
  /*padding: 0;*/
}

.page__footer .links .social-icons {
  padding-left: 0;
  text-align: left;
}
</style>

<div class="page__footer">
  <div class="links">
    <ul class="social-icons">
      <li style='display: inline-block; margin-right: 5px; font-style: bold'><strong>Links:</strong></li>
      <li style='display: inline-block; margin-right: 5px; font-style: normal;'><a href="https://accessibility.mit.edu"><i class="fa fa-fw fas fa-universal-access" aria-hidden="true"></i> Accessibility</a></li>
    </ul>
  </div>
</div>
</center>
</body>

</html>
