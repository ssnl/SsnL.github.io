<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en"><head>
  <title>Dataset Distillation Project Page</title>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

<meta property="og:image" content="./images/teaser.png"/>
<meta property="og:title" content="Dataset Distillation" />

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-167124286-1', 'auto');
ga('send', 'pageview');
</script>
<script src="lib.js" type="text/javascript"></script>
<script src="popup.js" type="text/javascript"></script>

<script type="text/javascript">
// redefining default features
var _POPUP_FEATURES = 'width=500,height=300,resizable=1,scrollbars=1,titlebar=1,status=1';
</script>
<link media="all" href="glab.css" type="text/css" rel="StyleSheet">
<style type="text/css" media="all">
IMG {
	PADDING-RIGHT: 0px;
	PADDING-LEFT: 0px;
	FLOAT: right;
	PADDING-BOTTOM: 0px;
	PADDING-TOP: 0px
}
#primarycontent {
	MARGIN-LEFT: auto; ; WIDTH: expression(document.body.clientWidth >
1000? "1000px": "auto" ); MARGIN-RIGHT: auto; TEXT-ALIGN: left; max-width:
1000px }
.primarycontent {
	TEXT-ALIGN: center
}
BODY {
  margin: 0px
}
</style>

<body>

<div id="primarycontent">
<center><h1 style="font-size: 275%">Dataset Distillation</h1></center>
<center><h2>
  <a href="..">Tongzhou Wang<sup>12</sup></a>&nbsp;&nbsp;&nbsp;
  <a href="https://people.csail.mit.edu/junyanz/">Jun-Yan Zhu<sup>2</sup></a>&nbsp;&nbsp;&nbsp;
  <a href="https://web.mit.edu/torralba/www/">Antonio Torralba<sup>2</sup></a>&nbsp;&nbsp;&nbsp;
  <a href="https://www.eecs.berkeley.edu/~efros/">Alexei A. Efros<sup>3</sup></a>&nbsp;&nbsp;&nbsp;
</h2></center>
<center><h2 style="font-size: 130%">
  <sup>1</sup>Facebook AI Research&nbsp;&nbsp;&nbsp;
  <sup>2</sup>MIT CSAIL&nbsp;&nbsp;&nbsp;
  <sup>3</sup>UC Berkeley
</h2></center>
<center><h2><strong>
  <a href="https://arxiv.org/abs/1811.10959">Paper</a> |
  <a href="https://github.com/SsnL/dataset-distillation">PyTorch code</a>
</strong></h2></center>
<br>
<hr>
<center style='margin-bottom:15px'><h2><strong>The <u>Dataset Distillation</u> Task</strong></h2></center>
<p>
<strong><u>Dataset distillation</u></strong> is the task of synethesizing a small dataset such that models trained on it achieve high performance on the original large dataset.

A dataset distillation algorithm takes as <strong>input</strong> a large real  dataset to be distilled (training set), and <strong>outputs</strong> a small synthetic distilled dataset, which is evaluated via testing models trained on this distilled dataset on a <em>separate</em> real dataset (validation/test set).  A good small distilled dataset is not only useful in dataset understanding, but has various applications (e.g., continual learning, privacy, neural architecture search, etc.).

This task was first introduced in our 2018 paper <a href='https://arxiv.org/abs/1811.10959'><em>Dataset Distillation</em><span style='font-family:normal'> [Wang et al., '18]</span></a>, along with a proposed algorithm using backpropagation through optimization steps. Details of this paper is also presented on this webpage.
</p>

<!-- <p>
Since its introduction, dataset distillation has gained increasing attention in the research community, with more papers are being published each year. Some works refer to the task as <em>Dataset Condensation</em>, likely named after a 2020 follow-up paper to <em>Dataset Distillation</em> <a href="https://arxiv.org/abs/2006.05929"  style='font-family:normal'>[Zhao et. al., '18]</a>, despite using the exact same setting and goal as dataset distillation. Nonetheless, all these wonderful researches work towards improving dataset distillation and exploring its various applications.
</p> -->

<p>
In recent years (2019-now), dataset distillation has gained increasing attention in the research community, across many institutes and labs. More papers are now being published each year. These wonderful researches have been constantly improving dataset distillation and exploring its various variants and applications.
</p>

<h3>Our Dataset Distillation Papers:</h3>
<ul style='font-size:120%'>
    <li>
        <a style='font-family:normal;font-weight:normal' href='.'>T. Wang, J. Y. Zhu, A. Torralba, A. and A. A. Efros. "Dataset Distillation". 2018.</a>
    </li>
    <li>
        <a style='font-family:normal;font-weight:normal' href='https://georgecazenavette.github.io/mtt-distillation/'>G. Cazenavette, T. Wang, A. Torralba, A. A. Efros, J. Y. Zhu. "Dataset Distillation by Matching Training Trajectories". CVPR 2022.</a>
    </li>
    <li>
        <a style='font-family:normal;font-weight:normal' href='https://georgecazenavette.github.io/mtt-distillation/'>G. Cazenavette, T. Wang, A. Torralba, A. A. Efros, J. Y. Zhu. "Wearable ImageNet: Synthesizing Tileable Textures via Dataset Distillation". Workshop on Computer Vision for Fashion, Art, and Design at CVPR 2022.</a>
    </li>
</ul>


<h3>Other Works Related to Dataset Distillation (Partial List):</h3>
<ul style='font-size:120%'>
    <li>
        <a style='font-family:normal;font-weight:normal' href='https://arxiv.org/abs/2006.05929'>[Zhao et al., ICLR 2021]</a>: new method called <em>Dataset Condensation</em> optimizing a gradient-matching surrogate objective.
    </li>
    <li>
        <a style='font-family:normal;font-weight:normal' href='https://ai.googleblog.com/2021/12/training-machine-learning-models-more.html'>[Nguyen et al. NeurIPS 2021]</a>: distillation w.r.t. the infinite-width limit Neural Tangent Kernel.
    </li>
    <li>
        <a style='font-family:normal;font-weight:normal' href='https://arxiv.org/abs/2205.14959'>[Kim et al., ICML 2022]</a>: reparametrizing distilled dataset via multi-scale patches.
    </li>
    <li>
        <a style='font-family:normal;font-weight:normal' href='https://arxiv.org/abs/2110.07580'>[Jin et al. ICLR 2022]</a>: distill large-scale graph for graph neural networks.
    </li>
    <li>
        <a style='font-family:normal;font-weight:normal' href='https://arxiv.org/abs/2202.02916'>[Lee et al., ICML 2022]</a>: careful scheduling of class-wise and class-collective objectives.
    </li>
    <li>
        <a style='font-family:normal;font-weight:normal' href='https://eng.uber.com/generative-teaching-networks/'>[Such et al., ICML 2020]</a>: training a generator that outputs good synthetic trianing data, with application in Neural Architecture Search.
    </li>
    <li>
        <a style='font-family:normal;font-weight:normal' href='https://arxiv.org/abs/2206.02916'>[Deng et al., 2022]</a>: new reparametrization that improves distillation via simple backpropagation through optimization steps, with application in continual learning.
    </li>
</ul>
<p>
Please check out <a href='https://github.com/Guang000/Awesome-Dataset-Distillation'>this awesome collection of dataset distillation papers</a>, curated and maintained by Guang Li, Bo Zhao and Tongzhou Wang.
</p>

<p style="font-size:small">Last updated by Tongzhou Wang on August 7th 2022.</p>
<hr>
<center><h2><strong>
Check out our latest CVPR 2022 work:
  <a href="https://georgecazenavette.github.io/mtt-distillation/">Project</a> |
  <a href="https://arxiv.org/abs/2203.11932">Paper</a> |
  <a href="https://github.com/GeorgeCazenavette/mtt-distillation">PyTorch code</a>
</strong></h2></center>


  <center>
  <div width=800 style="display: inline-block">
    <a href="images/teaser.pdf">
      <img src="images/teaser.png" width="850">
    </a>
  </div>
  </center>

 <!-- <p> -->
<h2 style="margin-bottom: 5px">Abstract</h2>

<div style="font-size:14px; text-align: justify;"><p>
  Model distillation aims to distill the knowledge of a complex model into a simpler one.  In this paper, we consider an alternative formulation called <em class="ul">dataset distillation</em>: we keep the model fixed and instead attempt to distill the knowledge from a large training dataset into a small one.  The idea is to <em>synthesize</em> a small number of data points that do not need to come from the correct data distribution, but will, when given to the learning algorithm as training data, approximate the model trained on the original data.  For example, we show that it is possible to compress 60,000 MNIST training images into just 10 synthetic <em class="ul">distilled images</em> (one per class) and achieve close to original performance with only a few steps of gradient descent, given a fixed network initialization. We evaluate our method in various initialization settings and with different learning objectives. Experiments on multiple datasets show the advantage of our approach compared to alternative methods.
</p></div>

<a href="https://arxiv.org/pdf/1811.10959.pdf"><img style="float: left; padding: 10px; PADDING-RIGHT: 30px;" alt="paper thumbnail" src="images/paper_thumbnail.jpg" width=170></a>
<br>


<h2>Paper</h2>
<p><a href="https://arxiv.org/abs/1811.10959">arxiv 1811.10959</a>, 2018. </p>


<h2>Citation</h2>
<p>Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A. Efros. "Dataset Distillation", arXiv preprint, 2018.
<a href="bibtex.txt">Bibtex</a>
</p>


<h2>Code: <a href='https://github.com/SsnL/dataset-distillation'>GitHub</a> </h2>
<br>
<!--
<h2 align='center'> ICCV Spotlight Talk</h2>
<table border="0" align="center" cellspacing="0" cellpadding="20">
    <td align="center" valign="middle">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/AxrKVfjSBiA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</td>

</table> -->

<br>
<br>
<br>
<h2>Experiment Results</h2>
<p style="font-size: small; font-size: 14px; padding-top: 9px; padding-bottom: 15px; color: #454545">Standard devaiations mentioned below are calculated on 200 held-out models.</p>
<h3 align='left'>Train networks with <em class="ul">a fixed known initialization</em></h3>
<table border="0" cellspacing="0" cellpadding="0">
  <tr>
    <td align="center" valign="middle" width=500>
        <h2>MNIST</h2>
        <p>10 images train test accuracy from 12.9% to 93.8%</p>
    </td>
    <td align="center" valign="middle" width=500>
        <h2>CIFAR10</h2>
        <p>100 images train test accuracy from 8.8% to 54.0%</p>
    </td>
  </tr>
  <tr>
    <td align="center" valign="middle" width=500><a href="images/fixed_mnist.png">
        <img class="result" src="images/fixed_mnist.png"> </a>
    </td>
    <td align="center" valign="middle" width=500>
        <a href="images/fixed_cifar.png">
            <img class="result" src="images/fixed_cifar.png"> </a>
    </td>
  </tr>
</table>
<p>&nbsp;</p>
<h3 align='left'>Train networks with <em class="ul">unknown random initializations</em></h3>
<table border="0" cellspacing="0" cellpadding="0">
  <tr>
    <td align="center" valign="middle" width=500>
        <h2>MNIST</h2>
        <p>100 images train test accuracy to 79.5% &#xb1; 8.1%</p>
    </td>
    <td align="center" valign="middle" width=500>
        <h2>CIFAR10</h2>
        <p>100 images train test accuracy to 36.8% &#xb1; 1.2%</p>
    </td>
  </tr>
  <tr>
    <td align="center" valign="middle" width=500><a href="images/random_mnist.png">
        <img class="result" src="images/random_mnist.png"  > </a>
    </td>
    <td align="center" valign="middle" width=500>
        <a href="images/random_cifar.png">
            <img class="result" src="images/random_cifar.png"  > </a>
    </td>
  </tr>
</table>
<p>&nbsp;</p>
<h3 align='left'>Adapt pre-trained networks with <em class="ul">unknown weights</em> to a new dataset</h3>
<table border="0" cellspacing="0" cellpadding="0">
  <tr>
    <td align="center" valign="middle" width=500>
        <h2>USPS &#10230 MNIST</h2>
        <p>100 images train test accuracy<br>
        from 67.5% &#xb1; 3.9% to 92.7% &#xb1; 1.4%</p>
    </td>
    <td align="center" valign="middle" width=500>
        <h2>SVHN &#10230 MNIST</h2>
        <p>100 images train test accuracy<br>
        from 51.6% &#xb1; 2.8% to 85.2% &#xb1; 4.7%</p>
    </td>
  </tr>
  <tr>
    <td align="center" valign="middle" width=500><a href="images/adapt_random_usps_mnist.png">
        <img class="result" src="images/adapt_random_usps_mnist.png"  > </a>
    </td>
    <td align="center" valign="middle" width=500>
        <a href="images/adapt_random_svhn_mnist.png">
            <img class="result" src="images/adapt_random_svhn_mnist.png" > </a>
    </td>
  </tr>
</table><br>
<p>&nbsp;</p>
<h3 align='left'>Attack well-trained classifiers with <em class="ul">unknown weights</em> within 1 gradient step</h3>
<table border="0" cellspacing="0" cellpadding="0">
  <tr>
    <td align="center" valign="middle" width=500>
        <h2>MNIST: <tt>0</tt> &#10230 <tt>1</tt></h2>
        <p>100 images train classifiers with 98.6% &#xb1; 0.5% test accuracy<br>
        to predict 71.4% &#xb1; 29.6% label <tt>0</tt> test images as label <tt>1</tt></p>
    </td>
    <td align="center" valign="middle" width=500>
        <h2>CIFAR10: <tt>plane</tt> &#10230 <tt>car</tt></h2>
        <p>100 images train classifiers with 78.2% &#xb1; 1.1% test accuracy<br>
        to predict 45.9% &#xb1; 18.1% label <tt>plane</tt> test images as label <tt>car</tt></p>
    </td>
  </tr>
  <tr>
    <td align="center" valign="middle" width=500><a href="images/attack_random_mnist.png">
        <img class="result" src="images/attack_random_mnist.png"  > </a>
    </td>
    <td align="center" valign="middle" width=500>
        <a href="images/attack_random_cifar.png">
            <img class="result" src="images/attack_random_cifar.png"  > </a>
    </td>
  </tr>
</table>
<p>&nbsp;</p>
<p>See our paper for more experiments, including adapting an AlexNet (pre-trained on ImageNet) to PASCAL-POC and CUB-200 with <em>only one image per class</em>.</p>

<br><br>
<h2>Related Work</h2>

<ul id='relatedwork'>
<li>
 Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. <a href="https://arxiv.org/abs/1503.02531"><strong>"Distilling the Knowledge in a Neural Network"</strong></a>, in NIPS Deep Learning Workshop 2014.
</li>
<li> Dougal Maclaurin, David Duvenaud, and Ryan Adams. <a href="https://arxiv.org/abs/1502.03492"><strong>"Gradient-based hyperparameter optimization through reversible learning"</strong></a>, in ICML 2015.
</li>

<li> Antonio Torralba and Alexei A Efros. <a href="http://people.csail.mit.edu/torralba/research/bias/"><strong>"Unbiased look at dataset bias"</strong></a>, in CVPR 2011.
</li>
<li> Agata Lapedriza, Hamed Pirsiavash, Zoya Bylinskii, and Antonio Torralba. <a href="https://arxiv.org/abs/1311.6510"><strong>"Are all training examples equally valuable?"</strong></a>, in arXiv preprint 2013.
</li>
</ul>

<br>
<h2>Acknowledgement</h2>
<p style="font-size:14px; text-align: justify; padding-bottom: 15px">This work was supported in part by NSF 1524817 on Advancing Visual Recognition with Feature Visualizations, NSF IIS-1633310, and Berkeley Deep Drive.</p>

</div>

<style type="text/css" media="all">
.page__footer {
  /*float: left;*/
  padding-top: 1em;
  padding-bottom: 0.5em;
  margin-left: 0;
  margin-right: 0;
  width: 100%;
  clear: both;
  /* sticky footer fix start */
  /*position: absolute;*/
  bottom: 0;
  height: auto;
  /* sticky footer fix end */
  margin-top: 3em;
  color: #898c8f;
  background-color: #f2f3f3;
  padding-left: 0em;
  padding-right: 0em;
  max-width: 100%;
}

.page__footer .links {
  margin-left: auto;
  margin-right: auto;
  max-width: 1000px;
  /*padding: 0;*/
}

.page__footer .links .social-icons {
  padding-left: 0;
  text-align: left;
}
</style>

<div class="page__footer">
  <div class="links">
    <ul class="social-icons">
      <li style='display: inline-block; margin-right: 5px; font-style: bold'><strong>Links:</strong></li>
      <li style='display: inline-block; margin-right: 5px; font-style: normal;'><a href="https://accessibility.mit.edu"><i class="fa fa-fw fas fa-universal-access" aria-hidden="true"></i> Accessibility</a></li>
    </ul>
  </div>
</div>

<div style="display:none">
<script type="text/javascript" src="http://gostats.com/js/counter.js"></script>
<script type="text/javascript">_gos='c3.gostats.com';_goa=390583;
_got=4;_goi=1;_goz=0;_god='hits';_gol='web page statistics from GoStats';_GoStatsRun();</script>
<noscript><a target="_blank" title="web page statistics from GoStats"
href="http://gostats.com"><img alt="web page statistics from GoStats"
src="http://c3.gostats.com/bin/count/a_390583/t_4/i_1/z_0/show_hits/counter.png"
style="border-width:0" /></a></noscript>
</div>

</body></html>
