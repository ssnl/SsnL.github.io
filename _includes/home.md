**I am...**{: style="font-size:1.05em;margin-right:1.1em"} a final-year PhD student at [MIT](https://www.csail.mit.edu/) with [Antonio Torralba](https://web.mit.edu/torralba/www/) and [Phillip Isola](https://web.mit.edu/phillipi/). I work on **machine learning**{: style="font-weight:600"}, **representation learning**{: style="font-weight:600"},  and **reinforcement learning**{: style="font-weight:600"}.

<!-- **Research.**{: style="font-size:1.05em;margin-right:1.1em"}  **Intelligence via learned data structures.**
{: style="text-align: justify;margin-bottom:0.35em"}

My approach towards intelligence is to utilize certain structures of data <span style="color:gray">(<em>e.g.,</em> <u>invariances</u> to impercecptible changes, <u>distances</u> w.r.t. decision-making abilities, task-specific <u>factorizations</u> of signal and noise)</span>. These features enable better task-solving, learning efficiency and generalization, but have no known mathematical formulas.
**How to characterize such structures?**{: style="font-weight:600"}&nbsp;
**How to optimize neural networks to extract and represent them?**{: style="font-weight:600"}&nbsp;
**How to use these <u>learned data structures</u> for better perception, decision-making, <em>etc</em>?**{: style="font-weight:600"}
{: style="text-align: justify; margin-bottom:0.35em;letter-spacing:-0.015em"}
 + <i class="fab fa-connectdevelop" style="color:#00B7EB;font-weight:bold;width:2em;text-align:center;padding-right:5px" aria-hidden="true" />Data structures **as** learned representations.
   <p style="margin-bottom:-4px"></p>
 + <i class="fas fa-robot" style="color:#DA70D6;font-weight:bold;width:2em;text-align:center;padding-right:5px" aria-hidden="true" />Data structures **for** efficient and general agents.
   <p style="margin-bottom:-4px"></p>
 + <i class="fas fa-database" style="color:#32CD32;font-weight:bold;width:2em;text-align:center;padding-right:5px" aria-hidden="true" />Dataset structures **of** the learning process, <span style="color:gray"><em>e.g.</em>, what makes for a good training set.</span> -->
   <!--- <p style="margin-bottom:-0.9em"></p> --->
<!---{: style="text-align: justify;padding-left:15px;margin-top:-1px;margin-bottom:10px;font-size:0.885em;list-style-type: none;"} --->
<!-- {: style="text-align: justify;padding-left:10px;margin-top:-1px;font-size:0.875em;list-style-type: none;margin-bottom:0.35em"} -->

**Research.**{: style="font-size:1.05em;margin-right:1.1em"} I am interested in the two intertwined paths towards **generalist agents**: (1) agents trained for multi-task (2) agents that can adaptively solve new tasks. 
My research learns structured representations that aggregate and select information about the world from various data sources, improving efficiency and generality of decision-making agents.
<!-- **inductive structures in machine learning and artificial intelligence**, with a focus on <u>useful structures for better perception and decision-making</u> and <u>learning such structures with neural nets</u> <span style="color:gray">(<em>e.g.,</em> invariances to imperceptible changes, distances w.r.t. decision-making capabilities, task-specific factorizations of signal and noise)</span>. -->
{: style="text-align: justify; margin-bottom:0.52em;letter-spacing:-0.015em"}

<!---Broadly, I am interested in representation learning, reinforcement learning, synthetic data, and [dataset distillation](./dataset_distillation/).--->
<!--- I frequently collaborate within and outside MIT, including with [Yuandong Tian](https://yuandong-tian.com/){:.color}, [Amy Zhang](https://amyzhang.github.io/), [Simon S. Du](https://simonshaoleidu.com/), [Alyosha Efros](https://people.eecs.berkeley.edu/~efros/), [Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/), etc. --->

**Outside research,**{: style="font-size:1.05em;margin-right:1.1em"} I spent my time on
**developing 1st offering of MIT's [Deep Learning course](https://phillipi.github.io/6.s898/)**{: style="font-weight:600"}, writing
**[![GitHub User's stars](https://img.shields.io/github/stars/ssnl?affiliations=OWNER%2CCOLLABORATOR&logo=github&label=stars){: style="height: 1.1em;vertical-align:text-bottom"}](/open-source/) [open-source](/open-source/) ML projects**{: style="font-weight:600"},
organizing **a NeurIPS [workshop](https://goal-conditioned-rl.github.io/2023/) on Goal-Conditional RL**{: style="font-weight:600"},
mentoring **[SGI](https://sgi.mit.edu/)**{: style="font-weight:600"} students (<a href="https://summergeometry.org/sgi2023/a-study-on-quasimetric-reinforcement-learning/">blog</a>),
<a style="margin-left:0px" href="/pro-bono-office-hours/">**pro bono office hours**{: style="font-weight:600"} (book me!)</a>, brewing coffee â˜•, travelling, 
and with ðŸ˜¸ðŸ˜¼.

<!--- **On 2023-2024 faculty job market:**{: style="font-size:1.05em;margin-right:1.1em"} YES âœ… --->

**Email:**{: style="font-size:1.05em;margin-right:1.1em"} `tongzhou _AT_ mit _DOT_ edu`
{: style="color:gray"}

<!--
In addition to research, my time at MIT is spent on
+ Collaboration outside MIT with [Yuandong Tian](https://yuandong-tian.com/){:.color}, [Amy Zhang](https://amyzhang.github.io/), [Simon S. Du](https://simonshaoleidu.com/), [Alyosha Efros](https://people.eecs.berkeley.edu/~efros/), [Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/), and many others.
  <p style="margin-bottom:-7px"></p>
+ **MIT's [Deep Learning](https://phillipi.github.io/6.s898/) course**: Developing its first offering and serving as the head TA.
  <p style="margin-bottom:-7px"></p>
+ <a style="margin-left:0px" href="https://github.com/ssnl">![GitHub User's stars](https://img.shields.io/github/stars/ssnl?affiliations=OWNER%2CCOLLABORATOR&logo=github&label=stars){: style="height: 1.1em;vertical-align:text-bottom"}</a> **Open source machine learning projects** (see [here](/open_source/) for a short list).
  <p style="margin-bottom:-7px"></p>
+ **Pro bono office hours:** Inspired by [Wei-Chiu Ma](https://people.csail.mit.edu/weichium/), I commit 1-2 hours per week to provide suggestions and/or mentorships to students from underrepresented groups or whoever is in need. Fill out [this form](https://forms.gle/pvrLmmrMkqAhApCC6) if you are interested.
  <p style="margin-bottom:-7px"></p>
{: style="text-align: justify;padding-left:1.2em;margin-top:-0.9em;font-size:0.88em;"}

Before MIT, I was an early member of the [PyTorch](https://pytorch.org/) team at [Meta AI (FAIR)](https://research.fb.com/category/facebook-ai-research-fair/) (2017-2019). My research journey started during undergradute years at UC Berkeley (2013-2017) with [Stuart Russell](http://people.eecs.berkeley.edu/~russell/){:.color}, [Ren Ng](https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html){:.color}, and [Alyosha Efros](https://people.eecs.berkeley.edu/~efros/){:.color} on probabilistic inference, graphics, and generative models.
{: style="text-align: justify;"} -->

## Selected Publications <span style="margin-left:6px;font-size:0.85em">([full list<i class="ai fa-fw ai-google-scholar-square" aria-hidden="true" />](https://scholar.google.com/citations?user=14HASnUAAAAJ))</span>
{: style="margin-bottom:0.2em; font-size:1.1em"}


<!-- 1. **Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning** (<i class="fab fa-connectdevelop" style="color:#00B7EB;font-weight:bold;padding-right:1px" aria-hidden="true" /><i class="fas fa-robot" style="color:#DA70D6;font-weight:bold" aria-hidden="true" />)<br />[<span class="small__tt">**ICML 2023**{: .colorful}</span>][[Project Page](./quasimetric_rl){: .small__tt}] [[arXiv](https://arxiv.org/abs/2304.01203){: .small__tt}] [<span>Code Coming Soon</span>{: .small__tt}] <br />
    **Tongzhou Wang**, Antonio Torralba, Phillip Isola, Amy Zhang
    {: style="margin-bottom:-0.35em"}

    <div class="table-like hide__mobile" style="justify-content:space-evenly;max-width:100%;width:100%;margin:auto;margin-top:calc(0.35em + 5px);padding: 0px;">
        <table style="max-width:750px;">
        <tr style="width: 100%;text-align: center;">
            <td style="font-size:1.25em;font-family:monospace;display: inline-block;text-align: center;width:35%;padding: 0px;border-bottom:0px">
            <img style="float: left; width: 100%;padding-bottom: 10px;" alt="paper thumbnail" src="./quasimetric_rl/assets/images/quasimetric_structure.png">
            Quasimetric Geometry
            </td><td style="font-size:2.9em;font-family:monospace;display: inline-block;text-align: center;width:5%;padding: 0px;border-bottom:0px">
            +
            </td><td style="font-size:1.25em;font-family:monospace;display: inline-block;text-align: center;vertical-align: bottom;width:33%;padding: 0px;border-bottom:0px;"><div style="padding-bottom: 3px" >
            <video src="https://user-images.githubusercontent.com/5674597/229619483-4e565dee-7b69-45a6-8f81-f21647f0df71.mp4" controls="controls" autoplay="true" loop="true" muted="muted" class="d-block rounded-bottom-2 border-top width-fit" style="width:100%">  </video></div>
            A Novel Objective<br>
            <div style="font-size: 0.67em;padding:0px;padding-top: 0px;">(Push apart <span style="color:rgb(217, 0, 0);font-weight: bold;">start state</span> and <span style="color:rgb(217, 0, 0);font-weight: bold;">goal</span><br>while maintaining local distances)</div>
            </td><td style="font-size:2.9em;font-family:monospace;display: inline-block;text-align: center;width:5%;padding: 0px;border-bottom:0px">
            =
            </td><td style="font-size:1.2em;font-family:monospace;display: inline-block;text-align: center;width:22%;padding: 0px;border-bottom:0px">
            Optimal Value $V^*$<br><span style="color:#97999c; font-weight: 200;font-style: italic;">AND</span><br>High-Performing<br>Goal-Reaching Agents
            </td>
        </tr>
        </table>
    </div>

1. **Improved Representation of Asymmetrical Distances with Interval Quasimetric Embeddings** (<i class="fab fa-connectdevelop" style="color:#00B7EB;font-weight:bold" aria-hidden="true" />)<br />[[<span class="small__tt">**NeurIPS 2022 NeurReps Workshop**{: .colorful}</span>](https://www.neurreps.org/)] [[Project Page](./interval_quasimetric_embedding){: .small__tt}] [[arXiv](https://arxiv.org/abs/2211.15120){: .small__tt}] [[PyTorch Package for Quasimetric Learning](https://github.com/quasimetric-learning/torch-quasimetric){: .small__tt}] <br />
    **Tongzhou Wang**, Phillip Isola
    {: style="margin-bottom:-0.35em"}

    <div>
    <img src="./interval_quasimetric_embedding/images/iqe_compute_nobg.png" alt="computing-iqe" class="hide__mobile" style="max-width:750px;width:100%;margin-top:0.55em">
    </div>

2. **Denoised MDPs: Learning World Models Better Than The World** (<i class="fab fa-connectdevelop" style="color:#00B7EB;font-weight:bold;padding-right:1px" aria-hidden="true" /><i class="fas fa-robot" style="color:#DA70D6;font-weight:bold" aria-hidden="true" />)<br />[<span class="small__tt">**ICML 2022**{: .colorful}</span>] [[Project Page](./denoised_mdp){: .small__tt}] [[arXiv](https://arxiv.org/abs/2206.15477){: .small__tt}] [[code](https://github.com/facebookresearch/denoised_mdp){: .small__tt}] <br />
    **Tongzhou Wang**, Simon S. Du, Antonio Torralba, Phillip Isola, Amy Zhang, Yuandong Tian
    {: style="margin-bottom:-0.35em"}

    <div>
    <video src="https://user-images.githubusercontent.com/5674597/173155667-d4bcc7af-1f12-4ba3-a733-ef9d5f631c96.mp4" controls="controls" autoplay="true" loop="true" muted="muted" class="d-block rounded-bottom-2 border-top width-fit hide__mobile" style="max-width:900px;width:115%;margin-top:-0.85em;margin-bottom:-0.75em;margin-left:-5%;position:sticky;z-index:-1">  </video>
    </div>

3. **On the Learning and Learnability of Quasimetrics** (<i class="fab fa-connectdevelop" style="color:#00B7EB;font-weight:bold;padding-right:1px" aria-hidden="true" /><i class="fas fa-robot" style="color:#DA70D6;font-weight:bold" aria-hidden="true" />)<br />[<span class="small__tt">**ICLR 2022**{: .colorful}</span>] [[Project Page](/quasimetric){: .small__tt}] [[arXiv](https://arxiv.org/abs/2206.15478){: .small__tt}] [[OpenReview](https://openreview.net/forum?id=y0VvIg25yk){: .small__tt}] [[code](https://github.com/SsnL/poisson_quasimetric_embedding){: .small__tt}] <br />
    **Tongzhou Wang**, Phillip Isola
    {: style="margin-bottom:-0.35em"}

    <div>
    <img src="./quasimetric/images/function_spaces_cropped.png" alt="quasimetric-function-spaces" class="hide__mobile" style="max-width:750px;width:100%">
    </div>

4. **Learning to See by Looking at Noise** (<i class="fab fa-connectdevelop" style="color:#00B7EB;font-weight:bold;padding-right:1px" aria-hidden="true" /><i class="fas fa-database" style="color:#32CD32;font-weight:bold;padding:1px" aria-hidden="true" />)<br />[<span class="small__tt">**NeurIPS 2021**{: .colorful}</span>] [[Project Page](https://mbaradad.github.io/learning_with_noise/){: .small__tt}] [[arXiv](https://arxiv.org/abs/2106.05963){: .small__tt}] [[code & datasets](https://github.com/mbaradad/learning_with_noise){: .small__tt}] <br />
    Manel Baradad\*, Jonas Wulff\*, **Tongzhou Wang**, Phillip Isola, Antonio Torralba
    {: style="margin-bottom:-0.35em"}

    <div>
    <img src="https://mbaradad.github.io/learning_with_noise/images/teaser.jpeg" alt="learning-to-see-by-looking-at-noises" class="hide__mobile" style="max-width:750px;width:100%;margin-top:0.35em">
    </div>

5. **<span style="letter-spacing:-0.25px">Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere</span>** (<i class="fab fa-connectdevelop" style="color:#00B7EB;font-weight:bold" aria-hidden="true" />)<br />[<span class="small__tt">**ICML 2020**{: .colorful}</span>] [[Project Page](/hypersphere){: .small__tt}] [[arXiv](https://arxiv.org/abs/2005.10242){: .small__tt}] [[code](https://github.com/SsnL/align_uniform){: .small__tt}] <br />
    **Tongzhou Wang**, Phillip Isola
    {: style="margin-bottom:-0.35em"}

    <div class="hide__mobile" style="max-width:750px;width:100%">
    <div style="width:100%; display:inline-flex;margin-top:0.2em">
        <span style="width:40%;border-bottom: 0px;padding:0px;margin-right:2%;vertical-align: bottom;text-align: left;display:inline-block">
            <img style="max-width:100%;max-height:100%;" src="/assets/images/hypersphere_stl10_scatter_linear_output.png" alt="hypersphere_stl10_scatter_linear_output" />
        </span>
        <span style="width:58%;border-bottom: 0px;padding-left:0px;padding-right:0px;text-align:right;vertical-align: bottom;font-size:75%">
            <div style="font-size: 0.735em;display: inline-block;text-align:left;padding-bottom:1.05em;width:100%;max-height:100%;margin-top:3.5%" >
              <div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em">
        <pre style="margin: 0; line-height: 160%">
<span style="color: #888888"># bsz : batch size (number of positive pairs)</span>
<span style="color: #888888"># d   : latent dim</span>
<span style="color: #888888"># x   : Tensor, shape=[bsz, d]</span>
<span style="color: #888888">#       latents for one side of positive pairs</span>
<span style="color: #888888"># y   : Tensor, shape=[bsz, d]</span>
<span style="color: #888888">#       latents for the other side of positive pairs</span>
<span style="color: #008800; font-weight: bold">def</span> <span style="color: #0066BB; font-weight: bold">align_loss</span>(x, y, alpha<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">2</span>):
<span style="color: #008800; font-weight: bold">    return</span> (x <span style="color: #333333">-</span> y)<span style="color: #333333">.</span>norm(p<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">2</span>, dim<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">1</span>)<span style="color: #333333">.</span>pow(alpha)<span style="color: #333333">.</span>mean()<br/>
<span style="color: #008800; font-weight: bold">def</span> <span style="color: #0066BB; font-weight: bold">uniform_loss</span>(x, t<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">2</span>):
<span style="color: #008800; font-weight: bold">    return</span> torch<span style="color: #333333">.</span>pdist(x, p<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">2</span>)<span style="color: #333333">.</span>pow(<span style="color: #0000DD; font-weight: bold">2</span>)<span style="color: #333333">.</span>mul(<span style="color: #333333">-</span>t)<span style="color: #333333">.</span>exp()<span style="color: #333333">.</span>mean()<span style="color: #333333">.</span>log()</pre>
      </div>
              <div style="text-align: center; font-size: 1.35em"><a href='https://github.com/SsnL/align_uniform'>PyTorch implementation</a> of the alignment and uniformity losses</div>
            </div>
        </span>
      </div>
    </div>

1. **Dataset Distillation** (<i class="fas fa-database" style="color:#32CD32;font-weight:bold;padding:1px" aria-hidden="true" />)<br />[[Project Page](/dataset_distillation){: .small__tt}] [[arXiv](https://arxiv.org/abs/1811.10959){: .small__tt}] [[code](https://github.com/SsnL/dataset-distillation){: .small__tt}]  [[DD Papers](https://github.com/Guang000/Awesome-Dataset-Distillation){: .small__tt}] <br />
    **Tongzhou Wang**, Jun-Yan Zhu, Antonio Torralba, Alexei A. Efros
    {: style="margin-bottom:-0.35em"}

    <div>
    <img src="/assets/images/dataset_distillation_fixed_mnist.png" alt="dataset_distillation_fixed_mnist" class="hide__mobile" style="max-width:750px;width:100%;margin-top:0.3em">
    </div>

2. **Meta-Learning MCMC Proposals**<br />[<span class="small__tt">**NeurIPS 2018**{: .colorful}</span>] [<span class="small__tt">**PROBPROG 2018**{: .colorful}</span>] [[ICML 2017 AutoML Workshop Oral](./automl_17/slides.pdf){: .small__tt}] [[arXiv](https://arxiv.org/abs/1708.06040){: .small__tt}] <br />
    **Tongzhou Wang**, Yi Wu, David A. Moore, Stuart J. Russell
    {: style="margin-bottom:-0.35em"}

    <div>
    <img src="/assets/images/meta_learning_mcmc_gmm_trace.png" alt="meta_learning_mcmc_gmm_trace" class="hide__mobile" style="max-width:730px;width:100%;margin-top:0.55em">
    </div>

3. **Learning to Synthesize a 4D RGBD Light Field from a Single Image**<br />[<span class="small__tt">**ICCV 2017**{: .colorful}</span>] [[arXiv](https://arxiv.org/abs/1708.03292){: .small__tt}] <br />
    Pratul Srinivasan, **Tongzhou Wang**, Ashwin Sreelal, Ravi Ramamoorthi, Ren Ng
    {: style="margin-bottom:-0.35em"}

    <div>
    <img src="/assets/images/2d_to_4d_pipeline.png" alt="light-field-synthesis-pipeline" class="hide__mobile" style="max-width:750px;width:100%;margin-top:0.35em">
    </div>
{: style="text-align: justify;padding-left:20px;list-style-type: square;"} -->

<!--
## Selected Projects

1. **Improved Training of Cycle-Consistent Adversarial Networks**

    **Tongzhou Wang** and Yihan Lin with research group of Prof. Alexei A. Efros

    Related report: [**CycleGAN with Better Cycles**{: style="font-size: 0.95em"}](/better_cycles/report.pdf).

2. **Modeling Punctuations in Online Reviews**<br/>[[technical report](/punctuations/report.pdf){: .small__tt}]

    **Tongzhou Wang**

    ![light-field-synthesis-pipeline](/assets/images/punctuation_neg_ex.png){: style="max-height:7em;width:auto;"}
-->